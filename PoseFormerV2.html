<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="style_project_page.css" media="screen"/>
<link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">


<html lang="en">
<head>
	<title>PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation</title>
    <link rel="icon" type="image/png" href="research/PoseFormerV2/figures/Thinking_Face.jpg">

    <meta charset="UTF-8">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src=""></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <h1 class="project-title">
        PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation
    </h1>

    <div class="conference">
        In CVPR 2023
    </div>

    <br><br>


    <div class="authors">
        <a href=https://qitaozhao.github.io>
            Qitao Zhao <sup>1</sup>
        </a>
        <a href=https://zczcwh.github.io/>
            Ce Zheng <sup>2</sup>
        </a>
        <a href=https://humanperception.github.io/>
            Mengyuan Liu <sup>3</sup>
        </a>
        <a href=https://wangpichao.github.io/>
            Pichao Wang <sup>4</sup>
        </a>
        <a href=https://www.crcv.ucf.edu/chenchen//>
            Chen Chen <sup>2</sup>
        </a>
    </div>
    <br>

    <div class="affiliations">
        <span><sup>1</sup> Shandong University</span></br>
        <span><sup>2</sup> Center for Research in Computer Vision, University of Central Florida</span> </br>
        <span><sup>3</sup> Key Laboratory of Machine Perception, Peking University, Shenzhen Graduate School</span> </br>
        <span><sup>4</sup> Amazon Prime Video</span> </br>
    </div>
    <br><br>

    <div class="project-icons">
        <a href="https://arxiv.org/pdf/2303.17472.pdf">
            <i class="fa fa-file-pdf-o"></i> <br/>
            Paper
        </a>
        <a href="https://github.com/QitaoZhao/PoseFormerV2">
            <i class="fa fa-github"></i> <br/>
            Code <br/>
        </a>
        <a href="https://www.youtube.com/watch?v=2xVNrGpGldM&t=5s">
            <i class="fa fa-youtube-play"></i> <br/>
            Video
        </a>
       <!--  <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/658/POP_poster.pdf">
            <i class="fa fa-newspaper-o"></i> <br/>
            Poster
        </a> -->
       <!--  <a href="https://pop.is.tue.mpg.de/">
            <i class="fa fa-database"></i> <br/>
            Dataset
        </a> -->

    </div>

    <div class="teaser">
        <br>
        <p style="width: 90%; text-align: center;">
            We introduce <b>PoseFormerV2</b> which improves <a href=https://arxiv.org/pdf/2103.10455.pdf>PoseFormer</a> from two aspects: (a) the <b>efficiency</b> in processing long input sequences; (b) the <b>robustness</b> to noisy 2D joint detection, via a frequency-domain sequence representation.
        </p>
        <br>
        <img src="./research/PoseFormerV2/figures/framework.jpg" alt="Teaser figure."/>
        <!-- <br> -->

    </div>

    <br><br>
    
    <hr>
    <h1>Abstract</h1>

    <p style="width: 85%">
        Recently, transformer-based methods have gained significant success in sequential 2D-to-3D lifting human pose estimation. As a pioneering work, PoseFormer captures spatial relations of human joints in each video frame and human dynamics across frames with cascaded transformer layers and has achieved impressive performance. However, in real scenarios, the performance of PoseFormer and its follow-ups is limited by two factors: (a) The length of the input joint sequence; (b) The quality of 2D joint detection. Existing methods typically apply self-attention to all frames of the input sequence, causing a huge computational burden when the frame number is increased to obtain advanced estimation accuracy, and they are not robust to noise naturally brought by the limited capability of 2D joint detectors. In this paper, we propose PoseFormerV2, which exploits a compact representation of lengthy skeleton sequences in the frequency domain to efficiently scale up the receptive field and boost robustness to noisy 2D joint detection. With minimum modifications to PoseFormer, the proposed method effectively fuses features both in the time domain and frequency domain, enjoying a better speed-accuracy trade-off than its precursor. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that the proposed approach significantly outperforms the original PoseFormer and other transformer-based variants. </p>
    <br>
    </p>

    <hr>
    <h1>Quantitative Results</h1>
    <img src="./research/PoseFormerV2/figures/quantitative_results.jpg" style="width: 85%" alt="PoseFormerV2 features."/></br>
    <p style="width: 85%">
        Our method outperforms PoseFormerV1 [1] (left) and other transformer-based methods (right) in terms of speed-accuracy trade-off. RF denotes Receptive Field and k×RF indicates that the ratio between the full sequence length and the number of frames as input into the spatial encoder of PoseFormerV2 is k, i.e., the RF of the spatial encoder is expanded by k× with a few low-frequency coefficients of the full sequence.</p></br>

    <h1>Qualitative Results</h1>
    <img src="./research/PoseFormerV2/figures/visualization.jpg" style="width: 85%" alt="PoseFormerV2 features."/></br>
    <p style="width: 85%">
        Qualitative results of PoseFormerV2 under challenging in-the-wild images: (a) Occlusions; (b)(c) Missed 2D joint detection; (d) Switched 2D joints. We highlight the unreliable 2D detection with light-yellow circles and corresponding 3D pose estimations with orange circles. PoseFormerV2 shows great robustness to imperfect 2D joint detection.</p></br>

    <img src="./research/PoseFormerV2/figures/noise_comparison.jpg" style="width: 85%" alt="PoseFormerV2 features."/>
    <p style="width: 85%">
        Qualitative comparisons of PoseFormerV2 with MHFormer [2] and PoseFormerV1 [1]. We randomly add Gaussian noise to the 2D detection of a specific joint. We highlight the deviated 2D detection with light-yellow arrows and corresponding 3D pose estimations with orange arrows. PoseFormerV2 shows better robustness to highly noisy input than existing methods.</p></br>

    <hr>
    <h1>Video</h1>


    <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/2xVNrGpGldM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div></br>

    <p style="width: 85%">
        In the following, we provide the demo at the end of the introduction video. Here we add strong Gaussian noise to the detected 2D human pose and our method shows a suprisingly good <b>temporal consistency</b> under highly unreliable 2D detection.</p>
    <img src="./research/PoseFormerV2/figures/basketball.gif" style="width:85%;" height="40%">
<!--     <div class="video-container" style="padding-top: 28%;">
        <iframe 
                src="https://media.githubusercontent.com/media/QitaoZhao/QitaoZhao.github.io/main/research/PoseFormerV2/figures/basketball.mp4" 
                frameBorder="0" 
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div> -->

    <!-- <p style="width: 90%; text-align: center;font-size: 14pt;">
    </br>
        中国大陆的朋友可<a href='https://www.bilibili.com/video/BV1KQ4y1z7Sc?share_source=copy_web'>在B站观看</a> | The video is also available on <a href='https://www.bilibili.com/video/BV1KQ4y1z7Sc?share_source=copy_web'>Bilibili</a> from mainland China
    </p> -->


    <br>

    <hr>
    <h1>Paper</h1>
       <!-- <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.pdf"> -->
        <img src="research/PoseFormerV2/figures/paper_snapshot_1x8.jpg" style="width:85%;" height="auto"></a>

       <div class="paper-info">
       <br>
       <span style="font-size: 14pt; font-weight: bold;">PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation</span><br>
       <span style="font-size: 14pt;"> Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, Chen Chen. </span>  <br>
       <span style="font-size: 14pt;">In CVPR 2023</span> 
       <!-- <br> -->
   <!--     <span style="font-size: 14pt;">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[Paper (CVF Version)]</a>&nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Ma_The_Power_of_ICCV_2021_supplemental.pdf" target="_blank" rel="noopener">[Supp]</a>&nbsp;
       <a href="https://arxiv.org/abs/2109.01137" target="_blank" rel="noopener">[arXiv]<br /></a> -->
    
       <pre><code>@inproceedings{zhao2023poseformerv2,
    title={PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation},
    author={Zhao, Qitao and Zheng, Ce and Liu, Mengyuan and Wang, Pichao, and Chen, Chen},
    booktitle={Conference on Computer Vision and Pattern Recognition 2023},
    year={2023},
}
</code></pre>
</div>

    <br><br>

    <hr>
    <h1>References</h1>
    <p style="width: 85%;text-align:left; ">
        [1] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. <b>3d human pose estimation with spatial and temporal transformers</b>. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11656–11665, October 2021.<br>

        [2] Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool. <b>Mhformer: Multi-hypothesis transformer for 3d human pose estimation</b>. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13147–13156, June 2022.<br><br>

    <!-- <b><a href="https://cape.is.tue.mpg.de/">Learning to Dress 3D People in Generative Clothing (CVPR 2020)</a></b> <br>
    <i>Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J. Black</i><br>
    CAPE &mdash; a generative model and a large-scale dataset for 3D clothed human meshes in varied poses and garment types. 
    We trained POP using the <a href="https://cape.is.tue.mpg.de/dataset">CAPE dataset</a>, check it out! -->

    </p>

    <!-- <br><br> -->
    <hr>
    <h1>Acknowledgements</h1>

    <p style="width: 85%;">
        The work was done while Qitao was a research intern mentored by Chen Chen.
        Qitao acknowledges the insightful advices from co-authors and CVPR'23 reviewers.
        The webpage template is adapted from 
        <a href="https://qianlim.github.io/POP">POP</a>.
    
    </p>

    <br><br>
</div>

</body>

</html>