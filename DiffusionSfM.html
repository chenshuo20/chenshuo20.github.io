<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion">
  <meta name="keywords" content="Sparse view, reconstruction, diffusion, Structure-from-Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</title>


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YXDZHQRVSJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-YXDZHQRVSJ');
</script>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">DiffusionSfM: Predicting Structure and Motion<br>via Ray Origin and Endpoint Diffusion</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://qitaozhao.github.io/" target="_blank">Qitao Zhao</a>,</span>
            <span class="author-block">
              <a href="https://amyxlase.github.io/" target="_blank">Amy Lin</a>,</span>
            <span class="author-block">
              <a href="https://jefftan969.github.io/" target="_blank">Jeff Tan</a>,</span>
            <span class="author-block">
              <a href="https://jasonyzhang.com/" target="_blank">Jason Y. Zhang</a>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>,</span>
            <span class="author-block">
              <a href="https://shubhtuls.github.io/" target="_blank">Shubham Tulsiani</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CVPR 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.05473"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/QitaoZhao/DiffusionSfM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/qitaoz/DiffusionSfM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./research/DiffusionSfM/figures/teaser.png" alt="Teaser figure."/>
      <h5 class="">
        <strong>Top:</strong> Given a set of multi-view images (left), DiffusionSfM represents scene geometry and cameras (right) as pixel-wise ray origins and endpoints in a global frame. It learns a denoising diffusion model to infer these elements directly from multi-view inputs. Unlike traditional Structure-from-Motion (SfM) pipelines, which separate pairwise reasoning and global optimization into two stages, our approach unifies both into a single end-to-end multi-view reasoning framework. <strong>Bottom:</strong> Example results of inferred scene geometry and cameras for two distinct settings: a real-world outdoor scene (left) and a synthetic indoor scene (right).
      </h5>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-one">
          <video poster="" id="one" autoplay muted loop playsinline height="100%">
            <source src="./research/DiffusionSfM/videos/backpack_108_12850_22707_N5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-two">
          <video poster="" id="two" autoplay muted loop playsinline height="100%">
            <source src="./research/DiffusionSfM/videos/habitat_hm3d_train_00512-WZDzPCybQvS_WZDzPCybQvS.basis_N5_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-three">
          <video poster="" id="three" autoplay muted loop playsinline height="100%">
            <source src="./research/DiffusionSfM/videos/habitat_gibson_Elmira_N5_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-four">
          <video poster="" id="four" autoplay muted loop playsinline height="100%">
            <source src="./research/DiffusionSfM/videos/teddybear_159_17485_33236_N5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-five">
          <video poster="" id="five" autoplay muted loop playsinline height="100%">
            <source src="./research/DiffusionSfM/videos/bench_250_26758_54244_N5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            Given sparse multi-view images as input, DiffusionSfM predicts pixel-wise ray origins and endpoints in a global frame using a denoising diffusion process. For each image, we compute patch-wise embeddings with DINOv2 and embed noisy ray origins and endpoints into latents using a single downsampling convolutional layer, ensuring alignment with the spatial footprint of the image embeddings. We implement a Diffusion Transformer architecture that predicts clean ray origins and endpoints from noisy samples. A convolutional DPT head outputs full-resolution denoised ray origins and endpoints. To handle incomplete ground truth (GT) during training, we condition the model on a GT mask. At inference, the GT mask is set to all ones, enabling the model to predict origins and endpoints for all pixels. The predicted ray origins and endpoints can be directly visualized in 3D or post-processed to recover camera extrinsics, intrinsics, and multi-view consistent depth maps.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/framework.png" alt="Method figure."/>
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Interactive 3D Models</h2>
    <div class="columns is-centered">
      <div class="content has-text-justified">
          <div style="
          display: flex;
          justify-content: center;
          align-items: center;
          height: 500px;
          width: 100%;
          background: #ffffff;">
            <model-viewer id="QualitativeResult"
                          src="./research/DiffusionSfM/3D/kew_gardens_ruined_arch.glb"
                          alt="3D Model"
                          loading="eager"
                          touch-action="pan-y"
                          environment-image="legacy"
                          camera-orbit="180deg 70deg auto"
                          zoom-sensitivity="0.2"
                          camera-controls
                          style="width: 60%; height: 90%; background: #887c7c;">
            </model-viewer>
        </div>
        <div class="content has-text-justified">
          <div class="thumbnail-container" id="thumbnail-qualitative">
            <img src="./research/DiffusionSfM/3D/kew_gardens_ruined_arch.png" data-glb="./research/DiffusionSfM/3D/kew_gardens_ruined_arch.glb">
            <img src="./research/DiffusionSfM/3D/habitat_gibson_Divide_N3.png" data-glb="./research/DiffusionSfM/3D/habitat_gibson_Divide_N3.glb">
            <img src="./research/DiffusionSfM/3D/jellycat.png" data-glb="./research/DiffusionSfM/3D/jellycat.glb">
            <img src="./research/DiffusionSfM/3D/habitat_gibson_Chrisney_N5.png" data-glb="./research/DiffusionSfM/3D/habitat_gibson_Chrisney_N5.glb">
            <img src="./research/DiffusionSfM/3D/donut_366_39342_76295_N5.png" data-glb="./research/DiffusionSfM/3D/donut_366_39342_76295_N5.glb">
            <img src="./research/DiffusionSfM/3D/habitat_gibson_Kildare_N5.png" data-glb="./research/DiffusionSfM/3D/habitat_gibson_Kildare_N5.glb">
            <img src="./research/DiffusionSfM/3D/kotor_cathedral.png" data-glb="./research/DiffusionSfM/3D/kotor_cathedral.glb">  
          </div>  
        </div>  
        <style>
          .thumbnail-container img, .thumbnail-container video {
            transition: all 0.3s ease;
            border: 3px solid transparent;
            cursor: pointer;
          }
          .thumbnail-selected {
            transform: scale(1.2);
            border: 6px solid #79b4f2 !important; 
            box-shadow: 0 0 10px rgba(121, 180, 242, 0.5);
            z-index: 10;
            position: relative;
          }
          
          /* New styles for responsive horizontal gallery */
          .thumbnail-container {
            display: flex;
            flex-wrap: nowrap;
            overflow-x: auto;
            gap: 10px;
            padding: 10px 0;
            -webkit-overflow-scrolling: touch; /* Smooth scrolling on iOS */
            scrollbar-width: thin;
            align-items: center;
          }
          
          .thumbnail-container img, 
          .thumbnail-container video {
            flex: 0 0 auto;
            height: auto;
            width: 200px; /* Consistent width */
            object-fit: cover;
            max-width: none;
          }
          
          /* Custom scrollbar styling */
          .thumbnail-container::-webkit-scrollbar {
            height: 6px;
          }
          
          .thumbnail-container::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 10px;
          }
          
          .thumbnail-container::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 10px;
          }
          
          .thumbnail-container::-webkit-scrollbar-thumb:hover {
            background: #555;
          }
          
          /* Adjust for smaller screens */
          @media (max-width: 768px) {
            .thumbnail-container img,
            .thumbnail-container video {
              height: 100px;
            }
          }
        </style>
        <script>
          // The problem is here - you're trying to select an element that doesn't exist
          // document.querySelector('#thumbnail-qualitative img[src="resources/qualitative/college.png"]').classList.add('thumbnail-selected');
          
          // Instead, select the first element that actually exists in your thumbnail container
          document.addEventListener('DOMContentLoaded', function() {
            // Select the first element in the thumbnail container (video or image)
            const firstThumbnail = document.querySelector('#thumbnail-qualitative video, #thumbnail-qualitative img');
            if (firstThumbnail) {
              firstThumbnail.classList.add('thumbnail-selected');
              // If it's a video, play it
              if (firstThumbnail.tagName.toLowerCase() === 'video') {
                firstThumbnail.play();
              }
            }
            
            document.querySelectorAll('#thumbnail-qualitative img, #thumbnail-qualitative video').forEach(el => {
              // Rest of your click handler code remains the same
              el.addEventListener('click', () => {
                const glbSrc = el.getAttribute('data-glb');
                const modelViewer = document.getElementById('QualitativeResult');
                modelViewer.setAttribute('src', glbSrc);
                modelViewer.cameraOrbit = "180deg 70deg auto";
                modelViewer.resetTurntableRotation(0);
                modelViewer.jumpCameraToGoal();

                // Remove selection class from all elements
                document.querySelectorAll('#thumbnail-qualitative img, #thumbnail-qualitative video').forEach(element => {
                    element.classList.remove('thumbnail-selected');
                });
                
                // Add selection class to clicked element
                el.classList.add('thumbnail-selected');
                
                // Play video if it's a video element
                if (el.tagName.toLowerCase() === 'video') {
                    el.play();
                }
                
                // Pause all other videos
                document.querySelectorAll('#thumbnail-qualitative video').forEach(video => {
                    if (video !== el) {
                        video.pause();
                        video.currentTime = 0;
                    }
                });
              });
            });
          });
        </script>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <!-- Subsection. -->
        <h3 class="title is-4">Quantitative Comparison of Camera Pose Accuracy on CO3D</h3>
        <div class="content has-text-justified">
          <p>
            On the left, we report the proportion of relative camera rotations within 15Â° of the ground truth. On the right, we report the proportion of camera centers within 10% of the scene scale. To align the predicted camera centers to ground-truth, we apply an optimal similarity transform, hence the alignment is perfect at N=2 but worsens with more images. DiffusionSfM outperforms all other methods for camera center accuracy, and outperforms all methods trained on equivalent data for rotation accuracy.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/pose_acc.png" alt="SPARF Comparison figure." width="75%"/>
        </div>
        <!--/ Subsection. -->

        <!-- Subsection. -->
        <h3 class="title is-4">Quantitative Comparison of Predicted Geometry on CO3D Unseen Categories</h3>
        <div class="content has-text-justified">
          <p>
            Top: CD computed on all scene points. Bottom: CD computed on foreground points only. Models marked with "*" are trained on CO3D only, while those without are trained on multiple datasets. Note that top and bottom values are not directly comparable, as each ground-truth point cloud is individually normalized.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./research/DiffusionSfM/figures/geometry_acc.png" alt="LEAP Comparison figure." width="35%"/>
        </div>

        <!-- Subsection. -->
        <h3 class="title is-4">Quantitative Comparison of Camera Pose Accuracy on Two Scene-Level Datasets</h3>
        <div class="content has-text-justified">
          <p>
            Top: Habitat (2â€“5 views). Bottom: RealEstate10k (2, 4, 6, 8 views). Each grid reports camera rotation accuracy (left) and center accuracy (right). While DiffusionSfM performs on par with DUSt3R in rotation accuracy, it consistently surpasses DUSt3R in center accuracy.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/pose_acc_scene.png" alt="LEAP Comparison figure." width="35%"/>
        </div>

        <!-- Subsection. -->
        <h3 class="title is-4">Qualitative Comparison of Predicted Geometry and Camera Poses</h3>
        <div class="content has-text-justified">
          <p>
            DiffusionSfM demonstrates robust performance even with challenging inputs. Compared to DUSt3R, which sometimes fails to register images in a consistent manner, DiffusionSfM consistently yields a coherent global prediction. Additionally, while we observe that DUSt3R can predict highly precise camera rotations, it often struggles with camera centers (see the backpack example). Input images depicting scenes are out-of-distribution for RayDiffusion, as it is trained on CO3D only.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./research/DiffusionSfM/figures/vis_compare.jpg" alt="LEAP Comparison figure." width="100%"/>
        </div>

        <!-- Subsection. -->
        <div class="content has-text-centered">
            <img src="./research/DiffusionSfM/figures/vis_cameras.png" alt="LEAP Comparison figure." width="100%"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Discussion</h2>
    <p>
      We present DiffusionSfM and demonstrate that it recovers accurate predictions of both cameras and geometry from multi-view inputs. Although our results are promising, several challenges and open questions remain. 
    </p>
    <p>
      Notably, DiffusionSfM employs a pixel-space diffusion model, in contrast to the latent-space models adopted by state-of-the-art T2I generative systems. Operating in pixel space may require greater model capacity, yet our current model remains relatively small -- potentially explaining the noisy patterns observed along object boundaries. Learning an expressive latent space for ray origins and endpoints by training a VAE could be a promising direction for future work. In addition, the computational requirement in multi-view transformers scales quadratically with the number of input images: one would require masked attention to deploy systems like ours for a large set of input images. 
    </p>
    <p>
      Despite these challenges, we believe that our work highlights the potential of a unified approach for multi-view geometry tasks. We envision that our approach can be built upon to train a common system across related geometric tasks, such as SfM (input images with unknown origins and endpoints), registration (some images have known origins and endpoints, whereas others don't), mapping (known rays but unknown endpoints), and view synthesis (unknown pixel values for known rays). </p>
    </p>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@inproceedings{zhao2025diffusionsfm,
  title={DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion}, 
  author={Qitao Zhao and Amy Lin and Jeff Tan and Jason Y. Zhang and Deva Ramanan and Shubham Tulsiani},
  booktitle={CVPR},
  year={2025} 
}</code></pre>
  </div>
</section>


<section class="column" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>
    <p>
      We thank the members of the Physical Perception Lab at CMU for their valuable discussions, and extend special thanks to Yanbo Xu for his insights on diffusion models.
    </p>
    <p>
      This work used Bridges-2 at Pittsburgh Supercomputing Center through allocation CIS240166 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. This work was supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2505.05473">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/QitaoZhao/DiffusionSfM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built using this template (<a href="https://github.com/nerfies/nerfies.github.io">source code</a>). The 3D model visualization is adapted from <a href="https://vgg-t.github.io/">VGGT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
